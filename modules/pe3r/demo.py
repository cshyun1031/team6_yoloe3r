import math
import copy
import gradio as gr
import os
import torch
import numpy as np
import functools
import trimesh
from PIL import Image
from scipy.spatial.transform import Rotation
import requests
from io import BytesIO
import cv2
import matplotlib.pyplot as pl
import glob
from copy import deepcopy
import json

# Custom Modules
from modules.pe3r.images import Images
from modules.dust3r.inference import inference
from modules.dust3r.image_pairs import make_pairs
from modules.dust3r.utils.image import load_images, rgb
from modules.dust3r.utils.device import to_numpy
from modules.dust3r.viz import add_scene_cam, CAM_COLORS, cat_meshes, pts3d_to_trimesh
from modules.dust3r.cloud_opt import global_aligner, GlobalAlignerMode

# User API Modules
from modules.llm_final_api.main_report import main_report
from modules.llm_final_api.main_new_looks import main_new_looks
from modules.llm_final_api.main_modify_looks import main_modify_looks
from modules.IR.listup import listup

# -----------------------------------------------------------------------------
# 1. ì‹œê°í™” ë° ì§€ì˜¤ë©”íŠ¸ë¦¬ í—¬í¼ í•¨ìˆ˜
# -----------------------------------------------------------------------------

def _convert_scene_output_to_glb(outdir, imgs, pts3d, mask, focals, cams2world, cam_size=0.05,
                                 cam_color=None, as_pointcloud=False,
                                 transparent_cams=False, silent=False):
    """
    3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œì™€ ì¹´ë©”ë¼ ì •ë³´ë¥¼ Trimesh Sceneìœ¼ë¡œ êµ¬ì„±í•˜ê³ ,
    ì¢Œí‘œê³„ë¥¼ ë·°ì–´ì— ë§ê²Œ ë³€í™˜í•˜ì—¬ GLB íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜.
    """
    assert len(pts3d) == len(mask) <= len(imgs) <= len(cams2world) == len(focals)
    pts3d = to_numpy(pts3d)
    imgs = to_numpy(imgs)
    focals = to_numpy(focals)
    cams2world = to_numpy(cams2world)

    scene = trimesh.Scene()

    # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
    if as_pointcloud:
        pts = np.concatenate([p[m] for p, m in zip(pts3d, mask)])
        col = np.concatenate([p[m] for p, m in zip(imgs, mask)])
        pct = trimesh.PointCloud(pts.reshape(-1, 3), colors=col.reshape(-1, 3))
        scene.add_geometry(pct)
    else:
        meshes = []
        for i in range(len(imgs)):
            meshes.append(pts3d_to_trimesh(imgs[i], pts3d[i], mask[i]))
        mesh = trimesh.Trimesh(**cat_meshes(meshes))
        scene.add_geometry(mesh)

    # ì¹´ë©”ë¼ ì‹œê°í™” ì¶”ê°€
    for i, pose_c2w in enumerate(cams2world):
        if isinstance(cam_color, list):
            camera_edge_color = cam_color[i]
        else:
            camera_edge_color = cam_color or CAM_COLORS[i % len(CAM_COLORS)]
        add_scene_cam(scene, pose_c2w, camera_edge_color,
                      None if transparent_cams else imgs[i], focals[i],
                      imsize=imgs[i].shape[1::-1], screen_width=cam_size)

    # ì¢Œí‘œê³„ ë³€í™˜: ì²« ë²ˆì§¸ ì¹´ë©”ë¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³ , Y-up ì¢Œí‘œê³„ë¡œ íšŒì „ ë° Zì¶• ì´ë™
    scene.apply_transform(np.linalg.inv(cams2world[0]))
    rot = np.eye(4)
    rot[:3, :3] = Rotation.from_euler('x', 180, degrees=True).as_matrix()
    scene.apply_transform(rot)
    translate = np.eye(4)
    translate[2, 3] = -2.0 
    scene.apply_transform(translate)

    outfile = os.path.join(outdir, 'scene.glb')
    if not silent:
        print('(exporting 3D scene to', outfile, ')')
    scene.export(file_obj=outfile)
    return outfile


def get_3D_model_from_scene(outdir, silent, scene, min_conf_thr=3, as_pointcloud=False, mask_sky=False,
                            clean_depth=False, transparent_cams=False, cam_size=0.05):
    """
    ì¬êµ¬ì„±ëœ Scene ê°ì²´ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ì „ì²˜ë¦¬(í•˜ëŠ˜ ë§ˆìŠ¤í‚¹, ê¹Šì´ ì •ë¦¬ ë“±)í•œ í›„
    GLB ë³€í™˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” ë˜í¼ í•¨ìˆ˜.
    """
    if scene is None:
        return None
    if clean_depth:
        scene = scene.clean_pointcloud()
    if mask_sky:
        scene = scene.mask_sky()

    rgbimg = scene.ori_imgs
    focals = scene.get_focals().cpu()
    cams2world = scene.get_im_poses().cpu()
    pts3d = to_numpy(scene.get_pts3d())
    scene.min_conf_thr = float(scene.conf_trf(torch.tensor(min_conf_thr)))
    msk = to_numpy(scene.get_masks())
    return _convert_scene_output_to_glb(outdir, rgbimg, pts3d, msk, focals, cams2world, as_pointcloud=as_pointcloud,
                                        transparent_cams=transparent_cams, cam_size=cam_size, silent=silent)

def mask_nms(masks, threshold=0.8):
    """
    ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ê°„ì˜ ì¤‘ë³µì„ ì œê±°í•˜ëŠ” NMS(Non-Maximum Suppression) í•¨ìˆ˜.
    ì‘ì€ ë§ˆìŠ¤í¬ê°€ í° ë§ˆìŠ¤í¬ì— ì¼ì • ë¹„ìœ¨ ì´ìƒ í¬í•¨ë˜ë©´ ì œê±°í•¨.
    """
    keep = []
    mask_num = len(masks)
    suppressed = np.zeros((mask_num), dtype=np.int64)
    for i in range(mask_num):
        if suppressed[i] == 1:
            continue
        keep.append(i)
        for j in range(i + 1, mask_num):
            if suppressed[j] == 1:
                continue
            intersection = (masks[i] & masks[j]).sum()
            if min(intersection / masks[i].sum(), intersection / masks[j].sum()) > threshold:
                suppressed[j] = 1
    return keep

def filter(masks, keep):
    """
    NMS ê²°ê³¼ ì¸ë±ìŠ¤(keep)ì— í•´ë‹¹í•˜ëŠ” ë§ˆìŠ¤í¬ë§Œ ë‚¨ê¸°ëŠ” í•„í„°ë§ í•¨ìˆ˜.
    """
    ret = []
    for i, m in enumerate(masks):
        if i in keep: ret.append(m)
    return ret

def mask_to_box(mask):
    """
    ì´ì§„ ë§ˆìŠ¤í¬ë¡œë¶€í„° ë°”ìš´ë”© ë°•ìŠ¤(x_min, y_min, x_max, y_max) ì¢Œí‘œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    """
    if mask.sum() == 0:
        return np.array([0, 0, 0, 0])
    
    rows = np.any(mask, axis=1)
    cols = np.any(mask, axis=0)
    
    top = np.argmax(rows)
    bottom = len(rows) - 1 - np.argmax(np.flip(rows))
    left = np.argmax(cols)
    right = len(cols) - 1 - np.argmax(np.flip(cols))
    
    return np.array([left, top, right, bottom])

def box_xyxy_to_xywh(box_xyxy):
    """
    [x_min, y_min, x_max, y_max] í¬ë§·ì˜ ë°•ìŠ¤ë¥¼ [x, y, w, h] í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜.
    """
    box_xywh = deepcopy(box_xyxy)
    box_xywh[2] = box_xywh[2] - box_xywh[0]
    box_xywh[3] = box_xywh[3] - box_xywh[1]
    return box_xywh

# -----------------------------------------------------------------------------
# 2. YOLO ë° Feature ì¶”ì¶œ ë¡œì§
# -----------------------------------------------------------------------------

def get_class_embedding(class_id, feature_dim=1024):
    """
    í´ë˜ìŠ¤ IDë¥¼ ì‹œë“œ(seed)ë¡œ ì‚¬ìš©í•˜ì—¬ ê³ ì •ëœ ëœë¤ ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    generator = torch.Generator().manual_seed(int(class_id) + 100) 
    embedding = torch.randn(feature_dim, generator=generator)
    embedding = embedding / embedding.norm(dim=-1, keepdim=True)
    return embedding

@torch.no_grad
def get_mask_and_class_from_yolo(seg_model, image_np, original_size, conf=0.25):
    """
    YOLO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ ë§ˆìŠ¤í¬ì™€ í´ë˜ìŠ¤ IDë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    ë„ˆë¬´ ì‘ì€ ê°ì²´ëŠ” í•„í„°ë§í•˜ê³ , NMSë¥¼ ì ìš©í•˜ì—¬ ì¤‘ë³µì„ ì œê±°í•¨.
    """
    results = seg_model.predict(image_np, conf=conf, retina_masks=True, verbose=False)
    
    sam_mask = []
    class_ids = []
    img_area = original_size[0] * original_size[1]

    if results[0].masks is not None:
        masks_data = results[0].masks.data
        cls_data = results[0].boxes.cls
        
        for i, mask in enumerate(masks_data):
            bin_mask = mask > 0.5
            if bin_mask.sum() / img_area > 0.002:
                sam_mask.append(bin_mask)
                class_ids.append(int(cls_data[i].item()))

    if len(sam_mask) == 0:
        return [], []

    sam_mask = torch.stack(sam_mask)
    class_ids = torch.tensor(class_ids)

    sorted_idx = torch.argsort(sam_mask.sum(dim=(1, 2)), descending=True)
    sorted_sam_mask = sam_mask[sorted_idx]
    sorted_class_ids = class_ids[sorted_idx]
    
    keep = mask_nms(sorted_sam_mask)
    
    ret_mask = filter(sorted_sam_mask, keep)
    ret_class_ids = [sorted_class_ids[i].item() for i in keep]

    return ret_mask, ret_class_ids


@torch.no_grad
def get_cog_feats(images, pe3r):
    """
    YOLOì™€ SAM2ë¥¼ ê²°í•©í•˜ì—¬ ë‹¤ì‹œì  ì´ë¯¸ì§€ì—ì„œ ì¼ê´€ëœ ê°ì²´ ë§ˆìŠ¤í¬ì™€ íŠ¹ì§• ë²¡í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ë¡œì§.
    ì²« í”„ë ˆì„ YOLO íƒì§€ -> SAM2 ë¹„ë””ì˜¤ ì¶”ì  -> ì¤‘ê°„ í”„ë ˆì„ YOLO ì¬íƒì§€ ë° ë³‘í•© ê³¼ì •ì„ ìˆ˜í–‰í•¨.
    """
    cog_seg_maps = []
    rev_cog_seg_maps = []
    
    inference_state = pe3r.sam2.init_state(images=images.sam2_images, video_height=images.sam2_video_size[0], video_width=images.sam2_video_size[1])
    mask_num = 0
    obj_id_to_class_id = {} 

    np_images = images.np_images
    np_images_size = images.np_images_size
    
    # ì²« í”„ë ˆì„ ê°ì²´ íƒì§€
    masks, class_ids = get_mask_and_class_from_yolo(pe3r.seg_model, np_images[0], np_images_size[0])
    
    for i, mask in enumerate(masks):
        _, _, _ = pe3r.sam2.add_new_mask(
            inference_state=inference_state,
            frame_idx=0,
            obj_id=mask_num,
            mask=mask,
        )
        obj_id_to_class_id[mask_num] = class_ids[i]
        mask_num += 1

    video_segments = {} 
    
    # SAM2 ë¹„ë””ì˜¤ ì „íŒŒ ë° ì¶”ê°€ ê°ì²´ íƒì§€
    for out_frame_idx, out_obj_ids, out_mask_logits in pe3r.sam2.propagate_in_video(inference_state):
        sam2_masks = (out_mask_logits > 0.0).squeeze(1)

        video_segments[out_frame_idx] = {
            out_obj_id: sam2_masks[i].cpu().numpy()
            for i, out_obj_id in enumerate(out_obj_ids)
        }

        if out_frame_idx == 0:
            continue

        yolo_masks, yolo_class_ids = get_mask_and_class_from_yolo(pe3r.seg_model, np_images[out_frame_idx], np_images_size[out_frame_idx])

        for i, yolo_mask in enumerate(yolo_masks):
            flg = 1
            for sam2_mask in sam2_masks:
                area1 = yolo_mask.sum()
                area2 = sam2_mask.sum()
                intersection = (yolo_mask & sam2_mask).sum()
                
                if min(intersection / area1, intersection / area2) > 0.25:
                    flg = 0
                    break
            
            if flg:
                video_segments[out_frame_idx][mask_num] = yolo_mask.cpu().numpy()
                obj_id_to_class_id[mask_num] = yolo_class_ids[i]
                mask_num += 1

    # Feature Vector ìƒì„±
    multi_view_clip_feats = torch.zeros((mask_num + 1, 1024))
    
    for obj_id in range(mask_num):
        if obj_id in obj_id_to_class_id:
            cls_id = obj_id_to_class_id[obj_id]
            multi_view_clip_feats[obj_id] = get_class_embedding(cls_id)
        else:
            multi_view_clip_feats[obj_id] = torch.zeros(1024)
            
    multi_view_clip_feats[mask_num] = torch.zeros(1024)

    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§µ ìƒì„±
    for now_frame in range(len(video_segments)):
        image = np_images[now_frame]
        rev_seg_map = -np.ones(image.shape[:2], dtype=np.int64)
        sorted_dict_items = sorted(video_segments[now_frame].items(), key=lambda x: np.count_nonzero(x[1]), reverse=False)
        for out_obj_id, mask in sorted_dict_items:
            if mask.sum() == 0: continue
            rev_seg_map[mask] = out_obj_id
        rev_cog_seg_maps.append(rev_seg_map)

        seg_map = -np.ones(image.shape[:2], dtype=np.int64)
        sorted_dict_items_rev = sorted(video_segments[now_frame].items(), key=lambda x: np.count_nonzero(x[1]), reverse=True)
        for out_obj_id, mask in sorted_dict_items_rev:
            if mask.sum() == 0: continue
            box = np.int32(box_xyxy_to_xywh(mask_to_box(mask)))
            if box[2] == 0 and box[3] == 0: continue
            
            seg_map[mask] = out_obj_id
            
        cog_seg_maps.append(seg_map)

    return cog_seg_maps, rev_cog_seg_maps, multi_view_clip_feats


# -----------------------------------------------------------------------------
# 3. í•µì‹¬ 3D ì¬êµ¬ì„± í•¨ìˆ˜
# -----------------------------------------------------------------------------

def get_reconstructed_scene(outdir, pe3r, device, silent, filelist, schedule, niter, min_conf_thr,
                            as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size,
                            scenegraph_type, winsize, refid):
    """
    ì „ì²´ 3D ì¬êµ¬ì„± íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ëŠ” í•¨ìˆ˜.
    1. ì´ë¯¸ì§€ ë¡œë“œ -> 2. íŠ¹ì§• ì¶”ì¶œ -> 3. Dust3r ì¶”ë¡  -> 4. ê¸€ë¡œë²Œ ì •ë ¬ -> 5. ê²°ê³¼ ë°˜í™˜
    """
    if len(filelist) < 2:
        raise gr.Error("Please input at least 2 images.")

    images = Images(filelist=filelist, device=device)
    
    try:
        cog_seg_maps, rev_cog_seg_maps, cog_feats = get_cog_feats(images, pe3r)
        imgs = load_images(images, rev_cog_seg_maps, size=512, verbose=not silent)
    except Exception as e:
        print(f"Error in feature extraction: {e}")
        rev_cog_seg_maps = []
        for tmp_img in images.np_images:
            rev_seg_map = -np.ones(tmp_img.shape[:2], dtype=np.int64)
            rev_cog_seg_maps.append(rev_seg_map)
        cog_seg_maps = rev_cog_seg_maps
        cog_feats = torch.zeros((1, 1024))
        imgs = load_images(images, rev_cog_seg_maps, size=512, verbose=not silent)

    if len(imgs) == 1:
        imgs = [imgs[0], copy.deepcopy(imgs[0])]
        imgs[1]['idx'] = 1

    if scenegraph_type == "swin":
        scenegraph_type = scenegraph_type + "-" + str(winsize)
    elif scenegraph_type == "oneref":
        scenegraph_type = scenegraph_type + "-" + str(refid)

    # 1ì°¨ ì¶”ë¡  ë° ì •ë ¬
    pairs = make_pairs(imgs, scene_graph=scenegraph_type, prefilter=None, symmetrize=True)
    output = inference(pairs, pe3r.mast3r, device, batch_size=1, verbose=not silent)
    mode = GlobalAlignerMode.PointCloudOptimizer if len(imgs) > 2 else GlobalAlignerMode.PairViewer
    
    scene_1 = global_aligner(output, cog_seg_maps, rev_cog_seg_maps, cog_feats, device=device, mode=mode, verbose=not silent)
    lr = 0.01
    loss = scene_1.compute_global_alignment(tune_flg=True, init='mst', niter=niter, schedule=schedule, lr=lr)

    # 2ì°¨ ì •ë ¨(Refinement)
    try:
        import torchvision.transforms as tvf
        ImgNorm = tvf.Compose([tvf.ToTensor(), tvf.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        for i in range(len(imgs)):
            imgs[i]['img'] = ImgNorm(scene_1.imgs[i])[None]
        pairs = make_pairs(imgs, scene_graph=scenegraph_type, prefilter=None, symmetrize=True)
        output = inference(pairs, pe3r.mast3r, device, batch_size=1, verbose=not silent)
        mode = GlobalAlignerMode.PointCloudOptimizer if len(imgs) > 2 else GlobalAlignerMode.PairViewer
        scene = global_aligner(output, cog_seg_maps, rev_cog_seg_maps, cog_feats, device=device, mode=mode, verbose=not silent)
        ori_imgs = scene.ori_imgs
        lr = 0.01
        loss = scene.compute_global_alignment(tune_flg=False, init='mst', niter=niter, schedule=schedule, lr=lr)
    except Exception as e:
        scene = scene_1
        scene.imgs = ori_imgs
        scene.ori_imgs = ori_imgs
        print(f"Refinement failed, using initial scene: {e}")

    # ê²°ê³¼ ìƒì„±
    outfile = get_3D_model_from_scene(outdir, silent, scene, min_conf_thr, as_pointcloud, mask_sky,
                                      clean_depth, transparent_cams, cam_size)

    rgbimg = scene.imgs
    depths = to_numpy(scene.get_depthmaps())
    confs = to_numpy([c for c in scene.im_conf])
    cmap = pl.get_cmap('jet')
    depths_max = max([d.max() for d in depths]) if depths else 1
    depths = [d / depths_max for d in depths]
    confs_max = max([d.max() for d in confs]) if confs else 1
    confs = [cmap(d / confs_max) for d in confs]

    imgs = []
    for i in range(len(rgbimg)):
        imgs.append(rgbimg[i])
        imgs.append(rgb(depths[i]))
        imgs.append(rgb(confs[i]))

    return scene, outfile, imgs


def get_3D_object_from_scene(outdir, pe3r, silent, text, threshold, scene, min_conf_thr, as_pointcloud, 
                             mask_sky, clean_depth, transparent_cams, cam_size):
    """
    (Legacy) í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ YOLO-Worldë¡œ íŠ¹ì • ê°ì²´ë¥¼ ê²€ìƒ‰í•˜ê³ , 
    í•´ë‹¹ ê°ì²´ ì™¸ ì˜ì—­ì„ ì–´ë‘¡ê²Œ ì²˜ë¦¬í•˜ì—¬ 3D ëª¨ë¸ì„ ì¬ìƒì„±í•˜ëŠ” í•¨ìˆ˜.
    """
    if not hasattr(scene, 'backup_imgs'):
        scene.backup_imgs = [img.copy() for img in scene.ori_imgs]

    print(f"Searching for: '{text}' using YOLO-World...")

    search_classes = [text] 
    if hasattr(pe3r.seg_model, 'set_classes'):
        pe3r.seg_model.set_classes(search_classes)

    original_images = scene.backup_imgs 
    masked_images = []

    for i, img in enumerate(original_images):
        img_input = img.copy()
        if img_input.dtype != np.uint8:
            if img_input.max() <= 1.0:
                img_input = (img_input * 255).astype(np.uint8)
            else:
                img_input = img_input.astype(np.uint8)

        conf_thr = 0.05 
        
        results = pe3r.seg_model.predict(img_input, conf=conf_thr, retina_masks=True, verbose=False)
        
        combined_mask = np.zeros(img.shape[:2], dtype=bool)
        found = False

        if results[0].masks is not None:
            masks = results[0].masks.data.cpu().numpy()
            for mask in masks:
                if mask.shape != combined_mask.shape:
                    mask = cv2.resize(mask, (combined_mask.shape[1], combined_mask.shape[0]))
                combined_mask = np.logical_or(combined_mask, mask > 0.5)
                found = True
        
        if found:
            masked_img = img.copy()
            if img.dtype == np.uint8:
                masked_img[~combined_mask] = 30 
            else:
                masked_img[~combined_mask] = 0.1 
            masked_images.append(masked_img)
        else:
            masked_images.append(img * 0.1)

    scene.ori_imgs = masked_images
    scene.imgs = masked_images 

    outfile = get_3D_model_from_scene(outdir, silent, scene, min_conf_thr, as_pointcloud, mask_sky,
                                      clean_depth, transparent_cams, cam_size)
    return outfile

def highlight_selected_object(
    scene, mask_list, object_id_list,
    min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size,
    evt: gr.SelectData,
    outdir=None 
): 
    """
    UI ê°¤ëŸ¬ë¦¬ì—ì„œ ì„ íƒëœ ê°€êµ¬ ê°ì²´ë¥¼ íŒŒë€ìƒ‰ í‹´íŠ¸ë¡œ í•˜ì´ë¼ì´íŠ¸í•˜ê³  3D ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜.
    """
    if scene is None or not mask_list:
        print("âš ï¸ Scene or mask_list is empty.")
        return None

    if evt is None or not isinstance(evt, gr.SelectData):
        print(f"âš ï¸ Error: evt is {type(evt)}. Gradio failed to pass SelectData.")
        return None

    selected_index = evt.index
    print(f"ğŸ–±ï¸ Clicked index: {selected_index}")

    if selected_index >= len(object_id_list):
        print("Error: Index out of range")
        return None
        
    target_obj_id = object_id_list[selected_index] 
    print(f"ğŸ¯ [Highlight] Target Object: {target_obj_id}")

    if not hasattr(scene, 'backup_imgs'):
        scene.backup_imgs = [img.copy() for img in scene.ori_imgs]

    masked_images = []
    original_images = scene.backup_imgs
    
    alpha = 0.5 

    for i, img in enumerate(original_images):
        current_frame_masks = mask_list[i]
        
        target_mask = None
        if target_obj_id in current_frame_masks:
            target_mask = current_frame_masks[target_obj_id]
        
        img_h, img_w = img.shape[:2]
        processed_img = img.copy()
        
        if target_mask is not None:
            if target_mask.shape[:2] != (img_h, img_w):
                target_mask = cv2.resize(target_mask.astype(np.uint8), (img_w, img_h), interpolation=cv2.INTER_NEAREST).astype(bool)
            
            if processed_img.dtype == np.uint8:
                roi = processed_img[target_mask].astype(np.float32)
                blue_layer = np.array([0, 0, 255], dtype=np.float32) 
                
                blended = (roi * (1 - alpha)) + (blue_layer * alpha)
                processed_img[target_mask] = blended.astype(np.uint8)
            else:
                roi = processed_img[target_mask]
                blue_layer = np.array([0.0, 0.0, 1.0], dtype=processed_img.dtype)
                
                blended = (roi * (1 - alpha)) + (blue_layer * alpha)
                processed_img[target_mask] = blended
        
        masked_images.append(processed_img)

    scene.ori_imgs = masked_images
    scene.imgs = masked_images

    if outdir is None:
        print("Error: outdir is None")
        return None

    outfile = get_3D_model_from_scene(outdir, False, scene, min_conf_thr, as_pointcloud, mask_sky, 
                                      clean_depth, transparent_cams, cam_size)
    
    return outfile


def set_scenegraph_options(inputfiles, winsize, refid, scenegraph_type):
    """
    Gradio UIì—ì„œ Scene Graph ì„¤ì •(Swin/Oneref)ì— ë”°ë¼ ìŠ¬ë¼ì´ë” ê°€ì‹œì„±ì„ ì¡°ì •í•˜ëŠ” í•¨ìˆ˜.
    """
    num_files = len(inputfiles) if inputfiles is not None else 1
    max_winsize = max(1, math.ceil((num_files - 1) / 2))
    
    if scenegraph_type == "swin":
        return gr.update(visible=True, value=max_winsize, maximum=max_winsize), gr.update(visible=False)
    elif scenegraph_type == "oneref":
        return gr.update(visible=False), gr.update(visible=True, maximum=num_files - 1)
    else:
        return gr.update(visible=False), gr.update(visible=False)


# -----------------------------------------------------------------------------
# 4. Main Demo UI (Gradio ì¸í„°í˜ì´ìŠ¤)
# -----------------------------------------------------------------------------

def main_demo(tmpdirname, pe3r, device, server_name, server_port, silent=False):
    
    # Partial Functions ì„¤ì •
    recon_fun = functools.partial(get_reconstructed_scene, tmpdirname, pe3r, device, silent)
    model_from_scene_fun = functools.partial(get_3D_model_from_scene, tmpdirname, silent)
    get_3D_object_from_scene_fun = functools.partial(get_3D_object_from_scene, tmpdirname, pe3r, silent)

    # ì´ˆê¸° ìƒì„± ì‹œ 3D ëª¨ë¸(ìƒë‹¨ ë³€í˜•ë³¸, í•˜ë‹¨ ì›ë³¸)ê³¼ ê°¤ëŸ¬ë¦¬ë¥¼ ë™ì‹œì— ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ë˜í¼ í•¨ìˆ˜
    def initial_recon_wrapper(*args):
        scene_obj, model_path, gallery_imgs = recon_fun(*args)
        return (
            scene_obj, 
            model_path, # ìƒë‹¨ ëª¨ë¸(outmodel)ìš© ê²½ë¡œ
            model_path, # í•˜ë‹¨ ëª¨ë¸(orig_model_display)ìš© ê²½ë¡œ (ì›ë³¸)
            gallery_imgs
        )

    # ìŠ¤íƒ€ì¼ ì„ íƒ ì €ì¥ í•¨ìˆ˜
    def save_style_json(selected_style):
        data = {"selected_style": selected_style}
        try:
            with open("modules/llm_final_api/style_choice.json", "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ [Saved] style_choice.json: {data}")
        except Exception as e:
            print(f"âŒ [Error] ìŠ¤íƒ€ì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ìœ ì € ì„ íƒ(ì¶”ê°€/ì‚­ì œ/ë³€ê²½) ì €ì¥ í•¨ìˆ˜
    def save_user_choice_json(use_add, use_remove, use_change):
        data = {
            "use_add": use_add,
            "use_remove": use_remove,
            "use_change": use_change
        }
        try:
            with open("modules/llm_final_api/user_choice.json", "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ [Saved] user_choice.json: {data}")
        except Exception as e:
            print(f"âŒ [Error] ìœ ì € ì„ íƒ ì €ì¥ ì‹¤íŒ¨: {e}")

    # ë¦¬í¬íŠ¸ íŒŒì¼ ì½ê¸° í•¨ìˆ˜
    def read_report_file(filename="report_analysis_result.txt"):
        if os.path.exists(filename):
            try:
                with open(filename, "r", encoding="utf-8") as f:
                    return f.read()
            except Exception as e:
                return f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"
        return "âš ï¸ ë¶„ì„ ê²°ê³¼ íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    # ë¶„ì„ ì‹¤í–‰ ë° UI ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    def run_analysis_and_show_ui(input_files):
        image_paths = []
        if input_files:
            for f in input_files:
                path = f.name if hasattr(f, 'name') else f
                image_paths.append(path)
        
        if main_report:
            try:
                print(f"ğŸ“Š [Info] ì´ë¯¸ì§€ ë¶„ì„ ì‹œì‘ ({len(image_paths)}ì¥)...")
                main_report(image_paths) 
            except Exception as e:
                print(f"âŒ [Error] ë¶„ì„ ëª¨ë“ˆ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                return f"### ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ\n{str(e)}", gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)
        else:
            return "### ë¶„ì„ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨", gr.update(visible=False), gr.update(visible=False), gr.update(visible=False)

        report_text = read_report_file("report_analysis_result.txt")
        # 4ê°œì˜ output ë°˜í™˜ (ë¦¬í¬íŠ¸, ì•„ì½”ë””ì–¸1, ì•„ì½”ë””ì–¸2, ë²„íŠ¼ í™œì„±í™”)
        return report_text, gr.update(visible=True, open=True), gr.update(visible=True, open=True), gr.update(visible=True)
    
    # ìƒì„±ëœ ì´ë¯¸ì§€ ë¡œë“œ í—¬í¼ í•¨ìˆ˜
    def load_generated_images(module_func, module_name):
        if module_func:
            try:
                print(f"ğŸ¨ [Info] {module_name} ì‹¤í–‰ ì¤‘...")
                module_func()
            except Exception as e:
                print(f"âŒ [Error] {module_name} ì‹¤íŒ¨: {e}")
        else:
            print(f"âš ï¸ Error: {module_name} ëª¨ë“ˆì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        output_dir = os.path.join(os.getcwd(), "apioutput")
        if not os.path.exists(output_dir):
            return []

        files = glob.glob(os.path.join(output_dir, "*.[pP][nN][gG]")) + \
                glob.glob(os.path.join(output_dir, "*.[jJ][pP][gG]")) + \
                glob.glob(os.path.join(output_dir, "*.[jJ][pP][eE][gG]"))
        
        files.sort(key=os.path.getmtime, reverse=True)
        return files[:3]

    def generate_and_load_new_images():
        return load_generated_images(main_new_looks, "main_new_looks")

    def generate_and_load_modified_images():
        return load_generated_images(main_modify_looks, "main_modify_looks")
    
    # ì›ë³¸ Scene ë°±ì—… í•¨ìˆ˜
    def backup_original_scene(scene, input_files):
        saved_paths = []
        if input_files:
            for f in input_files:
                path = f.name if hasattr(f, 'name') else f
                saved_paths.append(path)
        print(f"ğŸ’¾ [Backup] Sceneê³¼ íŒŒì¼ {len(saved_paths)}ê°œê°€ ì›ë³¸ìœ¼ë¡œ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return scene, saved_paths
    
    # ì›ë³¸ ë¦¬í¬íŠ¸ ë°±ì—… í•¨ìˆ˜
    def backup_original_report(report_text):
        print("ğŸ’¾ [Backup] ë¶„ì„ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ë°±ì—… ì™„ë£Œ")
        return report_text

    # ì›ë³¸ ë³µêµ¬(Undo) í•¨ìˆ˜
    def restore_original_scene(orig_scene, orig_inputs, orig_report, min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size):
        if orig_scene is None:
            return gr.update(), gr.update(), gr.update(), "âš ï¸ ì €ì¥ëœ ì›ë³¸ì´ ì—†ìŠµë‹ˆë‹¤."
        
        if hasattr(orig_scene, 'backup_imgs'):
            print("ğŸ”„ [Restore] ë§ˆìŠ¤í‚¹ëœ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ìœ¼ë¡œ ë³µêµ¬ ì¤‘...")
            orig_scene.ori_imgs = [img.copy() for img in orig_scene.backup_imgs]
            orig_scene.imgs = [img.copy() for img in orig_scene.backup_imgs]
        
        restored_model_path = model_from_scene_fun(
            orig_scene, min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size
        )
        
        restored_report = orig_report if orig_report else "ğŸ”„ ì›ë³¸ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        print("â†©ï¸ [Restore] ì›ë³¸ Scene ë° ë¦¬í¬íŠ¸ ë˜ëŒë¦¬ê¸° ì™„ë£Œ")
        return orig_scene, restored_model_path, orig_inputs, restored_report

    # IR(ì´ë¯¸ì§€ ê²€ìƒ‰) ì‹¤í–‰ ë° ê°¤ëŸ¬ë¦¬ í‘œì‹œ í•¨ìˆ˜
    def run_and_display(input_files):
        if not input_files: return [], [], []
        
        url_dict, mask_list, ordered_ids = listup(input_files)
        gallery_data = []
        for _, url in url_dict.items():
            try:
                response = requests.get(url[0])
                image = Image.open(BytesIO(response.content))
                gallery_data.append((image, f"Model Name : {url[1]}"))
            except: continue
        return gallery_data, mask_list, ordered_ids
    
    # ê°¤ëŸ¬ë¦¬ ì„ íƒ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
    def on_gallery_select(scene, mask_data, id_list, conf, pc, sky, clean, trans, size, evt: gr.SelectData): 
        return highlight_selected_object(scene, mask_data, id_list, conf, pc, sky, clean, trans, size, evt, outdir=tmpdirname)

    # -------------------------------------------------------------------------
    # UI ë ˆì´ì•„ì›ƒ ì •ì˜
    # -------------------------------------------------------------------------

    with gr.Blocks(title="IF U Demo", fill_width=True) as demo:

        interior_styles = [
            "AI ì¶”ì²œ", "ëª¨ë˜ Modern Interior", "ë¯¸ë‹ˆë©€ë¦¬ì¦˜ Minimalist Interior", "ìŠ¤ì¹¸ë””ë‚˜ë¹„ì•„/ë¶ìœ ëŸ½ Scandinavian Home",
            "ì¸ë”ìŠ¤íŠ¸ë¦¬ì–¼ Industrial Loft", "í´ë˜ì‹ Classic Interior Design", "ëª¨ë˜ í´ë˜ì‹ Modern Classic Home",
            "ë¹ˆí‹°ì§€ Vintage Home Decor", "ë ˆíŠ¸ë¡œ Retro Style Interior", "ë‚´ì¶”ëŸ´/ì   Natural Zen Interior",
            "ì¬íŒ¬ë”” Japandi Style", "ëŸ¬ìŠ¤í‹± Rustic Farmhouse", "íŒœí•˜ìš°ìŠ¤ Modern Farmhouse",
            "ì…°ë¹„ ì‹œí¬ Shabby Chic Style", "ì•„ë¥´ë°ì½” Art Deco Design", "ë¯¸ë“œ ì„¼ì¶”ë¦¬ ëª¨ë˜ Mid-Century Modern Home",
            "ë³´í—¤ë¯¸ì•ˆ/ë³´í˜¸ Boho Chic Interior", "íŠ¸ë¡œí”¼ì»¬ Tropical Home Decor", "ì§€ì¤‘í•´/ìŠ¤í˜ì¸ Mediterranean Home",
            "í”„ë Œì¹˜ French Country Style", "ì»¨í…œí¬ëŸ¬ë¦¬ Contemporary Style", "ìŠ¤íŒ€í‘í¬ Steampunk Decor",
            "ê³ ë”• Gothic Interior", "í•˜ì´í…Œí¬ Hi-Tech Interior", "ê·¸ë¦¬ìŠ¤ ë¦¬ë°”ì´ë²Œ Greek Revival Interior",
            "ì•„ë¥´ëˆ„ë³´ Art Nouveau Interior", "ì½”ìŠ¤íƒˆ/í•´ì•ˆ Coastal Home Decor", "ìŠ¤ìœ„ìŠ¤ ìƒ¬ë ˆ Swiss Chalet Interior",
            "ì´ì§‘íŠ¸ Egyptian Home Decor", "ì   ì•„ì‹œì•„ Asian Zen Decor", "ë§¥ì‹œë©€ë¦¬ì¦˜ Maximalist Decor",
            "í‚¤ì¹˜ Kitsch Decor Style", "ë°”ì´ì˜¤í•„ë¦­ Biophilic Design Home", "ì»¬ëŸ¬ ë¸”ë¡ Color Block Interior",
            "ëª¨ë…¸í¬ë¡œë§¤í‹± Monochromatic Room", "íŒ ì•„íŠ¸ Pop Art Interior", "ê·¸ëœë””ì‹œ Grandmillennial Style",
            "ë§¤ì‹œí˜ë¦° Masculine Interior Design", "í˜ë¯¸ë‹Œ Feminine Room Decor"
        ]
        
        # State Variables
        scene = gr.State(None)
        original_scene = gr.State(None)        
        original_inputfiles = gr.State(None)
        original_report_text = gr.State(None) 
        mask_data_state = gr.State([])
        object_id_list_state = gr.State([])

        gr.Markdown("## ğŸ§Š IF U Demo")

        with gr.Row():
            # --- ì¢Œì¸¡ íŒ¨ë„ (ì…ë ¥ ë° ì„¤ì •) ---
            with gr.Column(scale=1, min_width=320):
                inputfiles = gr.File(file_count="multiple", label="Input Images")
                
                with gr.Accordion("âš™ï¸ Settings", open=False):
                    schedule = gr.Dropdown(["linear", "cosine"], value='linear', label="schedule")
                    niter = gr.Number(value=300, precision=0, label="num_iterations")
                    scenegraph_type = gr.Dropdown(
                        [("complete", "complete"), ("swin", "swin"), ("oneref", "oneref")],
                        value='complete', label="Scenegraph"
                    )
                    winsize = gr.Slider(value=1, minimum=1, maximum=1, step=1, visible=False)
                    refid = gr.Slider(value=0, minimum=0, maximum=0, step=1, visible=False)
                    min_conf_thr = gr.Slider(label="min_conf_thr", value=3.0, minimum=1.0, maximum=20)
                    cam_size = gr.Slider(label="cam_size", value=0.05, minimum=0.001, maximum=0.1)
                    as_pointcloud = gr.Checkbox(value=True, label="As pointcloud")
                    transparent_cams = gr.Checkbox(value=True, label="Transparent cameras")
                    mask_sky = gr.Checkbox(value=False, visible=False)
                    clean_depth = gr.Checkbox(value=True, visible=False)

                run_btn = gr.Button("3Dë¡œ ë³€í™˜", variant="primary", elem_classes=["primary-btn"])
                IR_btn = gr.Button("ë°°ì¹˜ëœ ê°€êµ¬ ì œí’ˆëª… ì°¾ê¸°", variant="primary", elem_classes=["primary-btn"], visible=False)
                revert_btn = gr.Button("â†©ï¸ ì›ë³¸ ë˜ëŒë¦¬ê¸°", variant="secondary")
                
                with gr.Accordion("ğŸ¨ ë¶„ì„ë¦¬í¬íŠ¸ ì ìš©", open=True, visible=False) as analysis_accordion:
                    add = gr.Checkbox(value=False, label="ê°€êµ¬ ë°°ì¹˜ ì œì•ˆ ë°˜ì˜í•´ë³´ê¸°")
                    delete = gr.Checkbox(value=False, label="ê°€êµ¬ ì œê±° ì œì•ˆ ë°˜ì˜í•´ë³´ê¸°")
                    change = gr.Checkbox(value=False, label="ê°€êµ¬ ë³€ê²½ ì œì•ˆ ë°˜ì˜í•´ë³´ê¸°")
                    run_suggested_change_btn= gr.Button("ê²°ê³¼ ìƒì„±", variant="primary")
                with gr.Accordion("ë°© ë¶„ìœ„ê¸° ë°”ê¿”ë³´ê¸°", open=True, visible=False) as analysis_accordion1:
                    style = gr.Dropdown(interior_styles, value ="AI ì¶”ì²œ" , label="style", interactive=True)
                    run_style_change_btn = gr.Button("ê²°ê³¼ ìƒì„±", variant="primary")

                # (Legacy) ìˆ¨ê²¨ì§„ í…ìŠ¤íŠ¸ ê²€ìƒ‰ UI
                with gr.Row(visible=False):
                    text_input = gr.Textbox(label="Query Text")
                    threshold = gr.Slider(label="Threshold", value=0.85)
                    find_btn = gr.Button("Find")

            # --- ìš°ì¸¡ íŒ¨ë„ (3D ë·°ì–´ ë° ê²°ê³¼) ---
            with gr.Column(scale=2):
                # ìƒë‹¨ ë·°ì–´ (í˜„ì¬ ìƒíƒœ)
                outmodel = gr.Model3D(label="3D Reconstruction Result", interactive=True, height="65vh")
                
                # í•˜ë‹¨ ë·°ì–´ (ì›ë³¸ ìƒíƒœ)
                orig_model_display = gr.Model3D(
                    label="Original Model (Reference)", 
                    interactive=True,
                    height="25vh",
                )
                
                analysis_output = gr.Markdown(
                    value="ì—¬ê¸°ì— ê³µê°„ ë¶„ì„ ê²°ê³¼ê°€ í‘œì‹œë©ë‹ˆë‹¤.",
                    label="ê³µê°„ ë¶„ì„ ë¦¬í¬íŠ¸",
                    elem_classes=["report-box"]
                )
                outgallery = gr.Gallery(visible=False)
            
            with gr.Column():
                result_gallery = gr.Gallery(
                    label="Detected Objects", 
                    columns=1, 
                    height="auto", 
                    object_fit="contain" 
                )
                
                IR_btn.click(
                    fn=run_and_display, 
                    inputs=[inputfiles], 
                    outputs=[result_gallery, mask_data_state, object_id_list_state] 
                )

        # ---------------------------------------------------------------------
        # ì´ë²¤íŠ¸ ì—°ê²°
        # ---------------------------------------------------------------------
        
        # 1. 3D ì¬êµ¬ì„± ì‹¤í–‰ (ìµœì´ˆ ì‹¤í–‰: ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± í¬í•¨)
        recon_event = run_btn.click(
            fn=initial_recon_wrapper,
            inputs=[inputfiles, schedule, niter, min_conf_thr, as_pointcloud,
                    mask_sky, clean_depth, transparent_cams, cam_size,
                    scenegraph_type, winsize, refid],
            outputs=[scene, outmodel, orig_model_display, outgallery]
        )
        
        recon_event.success(
            fn=backup_original_scene,
            inputs=[scene, inputfiles],
            outputs=[original_scene, original_inputfiles]
        ).then(
            fn=lambda: "â³ 3D ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ê³µê°„ ë¶„ìœ„ê¸°ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...", outputs=analysis_output
        ).then(
            fn=run_analysis_and_show_ui,
            inputs=[inputfiles],
            outputs=[analysis_output, analysis_accordion, analysis_accordion1, IR_btn]
        ).success(
            fn=backup_original_report,
            inputs=[analysis_output],
            outputs=[original_report_text]
        )

        # 2. ë˜ëŒë¦¬ê¸° (Undo)
        revert_btn.click(
            fn=restore_original_scene,
            inputs=[original_scene, original_inputfiles, original_report_text, 
                    min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size],
            outputs=[scene, outmodel, inputfiles, analysis_output]
        )

        # 3. ìŠ¤íƒ€ì¼ ë³€ê²½ ì‹¤í–‰ (ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì œì™¸)
        run_style_change_btn.click(
            fn=generate_and_load_new_images, inputs=None, outputs=inputfiles
        ).then(
            fn=recon_fun,
            inputs=[inputfiles, schedule, niter, min_conf_thr, as_pointcloud,
                    mask_sky, clean_depth, transparent_cams, cam_size,
                    scenegraph_type, winsize, refid],
            outputs=[scene, outmodel, outgallery]
        )
        # [ìˆ˜ì •ë¨] run_analysis_and_show_ui í˜¸ì¶œ ì œê±°ë¨

        # 4. ì œì•ˆ ì ìš© (ê°€êµ¬ ë³€ê²½ ë“±) ì‹¤í–‰ (ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì œì™¸)
        run_suggested_change_btn.click(
            fn=generate_and_load_modified_images, inputs=None, outputs=inputfiles
        ).then(
            fn=recon_fun,
            inputs=[inputfiles, schedule, niter, min_conf_thr, as_pointcloud,
                    mask_sky, clean_depth, transparent_cams, cam_size,
                    scenegraph_type, winsize, refid],
            outputs=[scene, outmodel, outgallery]
        )
        # [ìˆ˜ì •ë¨] run_analysis_and_show_ui í˜¸ì¶œ ì œê±°ë¨

        # 5. IR ê°¤ëŸ¬ë¦¬ ì„ íƒ ì¸í„°ë™ì…˜
        result_gallery.select(
            fn=on_gallery_select,
            inputs=[scene, mask_data_state, object_id_list_state, 
                    min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size],
            outputs=outmodel
        )

        # 6. (Legacy) í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë²„íŠ¼
        find_btn.click(fn=get_3D_object_from_scene_fun,
             inputs=[text_input, threshold, scene, min_conf_thr, as_pointcloud, mask_sky,
                     clean_depth, transparent_cams, cam_size],
             outputs=outmodel)

        # 7. ê¸°íƒ€ ì„¤ì • ë³€ê²½ ì´ë²¤íŠ¸
        style.change(fn=save_style_json, inputs=[style], outputs=None)
        add.change(fn=save_user_choice_json, inputs=[add, delete, change], outputs=None)
        delete.change(fn=save_user_choice_json, inputs=[add, delete, change], outputs=None)
        change.change(fn=save_user_choice_json, inputs=[add, delete, change], outputs=None)

        scenegraph_type.change(set_scenegraph_options, [inputfiles, winsize, refid, scenegraph_type], [winsize, refid])
        inputfiles.change(set_scenegraph_options, [inputfiles, winsize, refid, scenegraph_type], [winsize, refid])
        
        update_inputs = [scene, min_conf_thr, as_pointcloud, mask_sky, clean_depth, transparent_cams, cam_size]
        for elem in [min_conf_thr, cam_size, as_pointcloud, mask_sky, clean_depth, transparent_cams]:
            if isinstance(elem, gr.Slider): elem.release(fn=model_from_scene_fun, inputs=update_inputs, outputs=outmodel)
            else: elem.change(fn=model_from_scene_fun, inputs=update_inputs, outputs=outmodel)

    demo.launch(share=True, server_name=server_name, server_port=server_port)
